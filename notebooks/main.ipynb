{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 14 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import pandarallel\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.stats.multitest\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "pandarallel.pandarallel.initialize(nb_workers=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "Perform normal/abnormal binarization of factors in the sample based on clinical standard information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clinical_binary(sample: pd.Series):\n",
    "    sample['B_PRT16_U'] = 0 if sample['B_PRT16_U'] == 0 else 1\n",
    "    sample['B_GLU16_U'] = 0 if sample['B_GLU16_U'] == 0 else 1\n",
    "    sample['B_BLOOD16_U'] = 0 if sample['B_BLOOD16_U'] == 0 else 1\n",
    "    sample['B_HBA1C'] = 0 if sample['B_HBA1C'] < 5.6 else 1\n",
    "    sample['B_GLU0'] = 0 if sample['B_GLU0'] > 70 and sample['B_GLU0'] < 99 else 1\n",
    "    sample['B_BUN'] = 0 if sample['B_BUN'] > 70 and sample['B_BUN'] < 99 else 1\n",
    "    sample['B_ALBUMIN'] = 0 if sample['B_ALBUMIN'] > 3.58 and sample['B_ALBUMIN'] < 5.2 else 1\n",
    "    sample['B_T_BIL'] = 0 if sample['B_T_BIL'] < 1.2 else 1\n",
    "    sample['B_TCHL'] = 0 if sample['B_TCHL'] < 199 else 1\n",
    "    sample['B_HDL'] = 0 if sample['B_HDL'] > 40 and sample['B_HDL'] < 60 else 1\n",
    "    sample['B_LDL'] = 0 if sample['B_LDL'] < 100 else 1\n",
    "    sample['B_TG'] = 0 if sample['B_TG'] < 149 else 1\n",
    "    sample['B_WBC_B'] = 0 if sample['B_WBC_B'] > 4 and sample['B_WBC_B'] < 10 else 1\n",
    "    sample['B_PLAT'] = 0 if sample['B_PLAT'] > 150 and sample['B_PLAT'] < 370 else 1\n",
    "\n",
    "    if sample['B_SEX'] == 1:\n",
    "        sample['B_CREATINE'] = 0 if sample['B_CREATINE'] > 0.7 and sample['B_CREATINE'] < 1.2 else 1\n",
    "        sample['B_AST'] = 0 if sample['B_AST'] < 40 else 1\n",
    "        sample['B_ALT'] = 0 if sample['B_ALT'] < 41 else 1\n",
    "        sample['B_R_GTP'] = 0 if sample['B_R_GTP'] > 10 and sample['B_R_GTP'] < 71 else 1\n",
    "        sample['B_RBC_B'] = 0 if sample['B_RBC_B'] > 4.1 and sample['B_RBC_B'] > 5.6 else 1\n",
    "        sample['B_HB'] = 0 if sample['B_HB'] > 13 and sample['B_HB'] < 17 else 1\n",
    "        sample['B_HCT'] = 0 if sample['B_HCT'] > 39 and sample['B_HCT'] < 51 else 1\n",
    "\n",
    "    elif sample['B_SEX'] == 2:\n",
    "        sample['B_CREATINE'] = 0 if sample['B_CREATINE'] > 0.5 and sample['B_CREATINE'] < 0.9 else 1\n",
    "        sample['B_AST'] = 0 if sample['B_AST'] < 32 else 1\n",
    "        sample['B_ALT'] = 0 if sample['B_ALT'] < 33 else 1\n",
    "        sample['B_R_GTP'] = 0 if sample['B_R_GTP'] > 6 and sample['B_R_GTP'] < 42 else 1\n",
    "        sample['B_RBC_B'] = 0 if sample['B_RBC_B'] > 3.7 and sample['B_RBC_B'] > 4.7 else 1\n",
    "        sample['B_HB'] = 0 if sample['B_HB'] > 11 and sample['B_HB'] < 15 else 1\n",
    "        sample['B_HCT'] = 0 if sample['B_HCT'] > 33 and sample['B_HCT'] < 45 else 1\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-1. Lists disease groups and clinical information to be verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "basics = ['B_SEX']\n",
    "diseases = ['B_HTN', 'B_DM', 'B_THY', 'B_LIP', 'B_LCA',\n",
    "            'B_GCA', 'B_HCCCA', 'B_COLCA', 'B_PACA', 'B_UTCA',\n",
    "            'B_BRCA', 'B_THYCA', 'B_PROCA', 'B_GALLCA']\n",
    "clinicals = ['B_BMI', 'B_PRT16_U', 'B_GLU16_U', 'B_BLOOD16_U', 'B_HBA1C',\n",
    "             'B_GLU0', 'B_BUN', 'B_ALBUMIN', 'B_CREATINE', 'B_AST',\n",
    "             'B_T_BIL', 'B_ALT', 'B_TCHL', 'B_R_GTP', 'B_HDL',\n",
    "             'B_LDL', 'B_TG', 'B_WBC_B', 'B_RBC_B', 'B_HB',\n",
    "             'B_HCT', 'B_PLAT']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-2. The selected clinical information is binarized based on clinical standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/KOGES_BASE.csv')\n",
    "raw_data = raw_data.set_index('RID')\n",
    "raw_data = raw_data.replace([66666, 77777, 99999], np.nan)\n",
    "raw_data = raw_data.loc[:, basics + diseases + clinicals].copy()\n",
    "raw_data = raw_data.dropna()\n",
    "\n",
    "raw_data = raw_data.parallel_apply(clinical_binary, axis=1, result_type='expand')\n",
    "\n",
    "raw_data.loc[:, diseases] = raw_data.loc[:, diseases].replace([1, 2], [0, 1])\n",
    "\n",
    "raw_data.loc[:, 'Obesity'] = (raw_data.loc[:, 'B_BMI'] > 30).astype('int')\n",
    "raw_data = raw_data.drop('B_BMI', axis=1)\n",
    "raw_data = raw_data.drop('B_SEX', axis=1)\n",
    "\n",
    "clinicals.remove('B_BMI')\n",
    "diseases.append('Obesity')\n",
    "\n",
    "raw_data = raw_data.astype('bool') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Network constructing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the binarized clinical information, we analyze the correlation of each information according to the Novelty-clustering algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_ratio(x: pd.Series, GAM: pd.DataFrame):\n",
    "    a = GAM.loc[x['gene_1'], :]\n",
    "    b = GAM.loc[x['gene_2'], :]\n",
    "    if (x['n_1'] == 0) | (x['n_2'] == 0):\n",
    "        return 0\n",
    "    return (a * b).abs().sum() / min(x['n_1'], x['n_2'])\n",
    "\n",
    "def H_1(x: pd.Series, GAM: pd.DataFrame):\n",
    "    import numpy as np\n",
    "    a = GAM.loc[x['gene_1'], :]\n",
    "    dist = np.array([(a == 0).sum(), (a == 1).sum()])\n",
    "    dist = dist / GAM.shape[1]\n",
    "    dist[dist == 0] = 1\n",
    "    return (-dist * np.emath.logn(2, dist)).sum()\n",
    "\n",
    "def H_2(x: pd.Series, GAM: pd.DataFrame):\n",
    "    import numpy as np\n",
    "    a = GAM.loc[x['gene_2'], :]\n",
    "    dist = np.array([(a == 0).sum(), (a == 1).sum()])\n",
    "    dist = dist / GAM.shape[1]\n",
    "    dist[dist == 0] = 1\n",
    "    return (-dist * np.emath.logn(2, dist)).sum()\n",
    "\n",
    "def H_1_2(x: pd.Series, GAM: pd.DataFrame):\n",
    "    import numpy as np\n",
    "    a = GAM.loc[x['gene_1'], :]\n",
    "    b = GAM.loc[x['gene_2'], :]\n",
    "    dist = np.array([[((a == 0) & (b == 0)).sum(), ((a == 1) & (b == 0)).sum()],\n",
    "                     [((a == 0) & (b == 1)).sum(), ((a == 1) & (b == 1)).sum()]])\n",
    "    dist = dist.flatten() / GAM.shape[1]\n",
    "    dist[dist == 0] = 1\n",
    "    return (-dist * np.emath.logn(2, dist)).sum()\n",
    "\n",
    "def w(x: pd.Series, params=(15, 0.5)):\n",
    "    import numpy as np\n",
    "    f = max(x['freq_1'], x['freq_2'])\n",
    "    return 1 / (1 + np.exp(-params[0] * (f - params[1])))\n",
    "\n",
    "def weighted_entropy(x: pd.Series, GAM: pd.DataFrame):\n",
    "    import numpy as np\n",
    "    w = x['w']\n",
    "    a = GAM.loc[x['gene_1'], :]\n",
    "    b = GAM.loc[x['gene_2'], :]\n",
    "\n",
    "    dist_1 = np.array([sum(a == 0), sum(a == 1) / w])\n",
    "    dist_1 = dist_1 / dist_1.sum()\n",
    "    dist_1[dist_1 == 0] = 1\n",
    "    H_1 = (-dist_1 * np.emath.logn(2, dist_1)).sum()\n",
    "\n",
    "    dist_2 = np.array([sum(b == 0), sum(b == 1) / w])\n",
    "    dist_2 = dist_2 / dist_2.sum()\n",
    "    dist_2[dist_2 == 0] = 1\n",
    "    H_2 = (-dist_2 * np.emath.logn(2, dist_2)).sum()\n",
    "\n",
    "    dist_1_2 = np.array([[((a == 0) & (b == 0)).sum(), ((a == 1) & (b == 0)).sum() / w],\n",
    "                         [((a == 0) & (b == 1)).sum() / w, ((a == 1) & (b == 1)).sum() / w]])\n",
    "    dist_1_2 = dist_1_2.flatten() / dist_1_2.sum()\n",
    "    dist_1_2[dist_1_2 == 0] = 1\n",
    "    H_1_2 = (-dist_1_2 * np.emath.logn(2, dist_1_2)).sum()\n",
    "    \n",
    "    return (H_1_2, H_1 + H_2 - H_1_2)\n",
    "\n",
    "def ASC(x: pd.Series, pseudo_result: pd.DataFrame):\n",
    "    pseudo_result = pseudo_result.copy()\n",
    "    term1 = pseudo_result.loc[(pseudo_result.loc[:, 'gene_1'] == x['gene_1']) | (pseudo_result.loc[:, 'gene_2'] == x['gene_1']),\n",
    "                             'effect_size'].mean()\n",
    "    term2 = pseudo_result.loc[(pseudo_result.loc[:, 'gene_1'] == x['gene_2']) | (pseudo_result.loc[:, 'gene_2'] == x['gene_2']),\n",
    "                             'effect_size'].mean()\n",
    "    term3 = pseudo_result.loc[:, 'effect_size'].mean()\n",
    "    return term1 + term2 - term3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_count = 1000\n",
    "weight_params = (15, 0.15)\n",
    "fdr_alpha = 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the novelty-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metabolic_syndromes = ['B_LIP', 'Obesity']\n",
    "\n",
    "for disease in metabolic_syndromes:\n",
    "    # single disease\n",
    "    # alteration_matrix = raw_data.loc[raw_data.loc[:, disease], clinicals].copy()\n",
    "\n",
    "    # metabolic syndrome with complication\n",
    "    alteration_matrix = raw_data.loc[(raw_data.loc[:, ['B_HTN', 'B_DM', 'B_HTN', 'B_LIP', 'B_THY', 'Obesity']].sum(axis=1) >= 2) & raw_data.loc[:, disease], clinicals].copy()\n",
    "    alteration_matrix.index.name = ''\n",
    "    alteration_matrix = alteration_matrix.transpose()\n",
    "\n",
    "    result = pd.DataFrame(itertools.combinations(alteration_matrix.index, 2), columns=['gene_1', 'gene_2'])\n",
    "    result.loc[:, 'n_1'] = alteration_matrix.loc[result.loc[:, 'gene_1'], :].isin([1]).sum(axis=1).values\n",
    "    result.loc[:, 'n_2'] = alteration_matrix.loc[result.loc[:, 'gene_2'], :].isin([1]).sum(axis=1).values\n",
    "    result.loc[:, 'freq_1'] = result.loc[:, 'n_1'] / alteration_matrix.shape[1]\n",
    "    result.loc[:, 'freq_2'] = result.loc[:, 'n_2'] / alteration_matrix.shape[1]\n",
    "    result.loc[:, 'overlap_ratio'] = result.parallel_apply(overlap_ratio, GAM=alteration_matrix, axis=1)\n",
    "    \n",
    "    # drop pairs with 0 overlap ratio to decrease calculation time\n",
    "    result = result.loc[result.loc[:, 'overlap_ratio'] != 0, :].copy()\n",
    "    result = result.reset_index(drop=True)\n",
    "    result.loc[:, 'H_1'] = result.parallel_apply(H_1, GAM=alteration_matrix, axis=1)\n",
    "    result.loc[:, 'H_2'] = result.parallel_apply(H_2, GAM=alteration_matrix, axis=1)\n",
    "    result.loc[:, 'H_1_2'] = result.parallel_apply(H_1_2, GAM=alteration_matrix, axis=1)\n",
    "    result.loc[:, 'MI'] = result.loc[:, 'H_1'] + result.loc[:, 'H_2'] - result.loc[:, 'H_1_2']\n",
    "    result.loc[:, 'w'] = result.parallel_apply(w, params=weight_params, axis=1) # choose rho as the median value of frequency\n",
    "    result.loc[:, 'wH_1_2'], result.loc[:, 'wMI'] = result.parallel_apply(weighted_entropy, GAM=alteration_matrix, axis=1, result_type='expand').T.values\n",
    "    \n",
    "    print(f\"[{time.strftime('%c', time.localtime(time.time()))}]\\t Statistical testing\")\n",
    "    tmp = result.loc[:, ['gene_1', 'gene_2', 'w']].copy()\n",
    "    \n",
    "    for i in tqdm.tqdm(range(0, randomized_count)):\n",
    "        np.random.seed(i)\n",
    "        GAM_random = alteration_matrix.copy()\n",
    "        GAM_random.apply(lambda x: np.random.shuffle(x.values), axis=1)\n",
    "        tmp = pd.concat([tmp, pd.Series(tmp.parallel_apply(weighted_entropy, GAM=GAM_random, axis=1, result_type='expand').T.values[1])], axis=1)\n",
    "\n",
    "    result.loc[:, 'pvalue'] = (tmp.loc[:, 0].transpose() > result.loc[:, 'wMI']).sum() / randomized_count\n",
    "    result.loc[:, 'FDR'], result.loc[:, 'FDR_pvalue'] =\\\n",
    "        statsmodels.stats.multitest.fdrcorrection(result.loc[:, 'pvalue'], alpha=fdr_alpha)\n",
    "    result.loc[:, 'effect_size'] =result.loc[:, 'wMI'] - tmp.loc[:, 0].mean(axis=1)\n",
    "\n",
    "    result.loc[:, 'ASC'] = result.parallel_apply(ASC, pseudo_result=result, axis=1)\n",
    "    result.loc[:, 'EIS'] = result.loc[:, 'effect_size'] - result.loc[:, 'ASC']\n",
    "    result.loc[:, 'norm_EIS'] = result.loc[:, 'EIS'] / result.loc[:, 'wH_1_2']\n",
    "\n",
    "    result.to_csv(f\"{disease}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
